{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b49e82-5e94-43a3-8726-054e2b2c597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 40)\n",
      "(1218,)\n",
      "(1218, 40, 1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 69\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_training_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#Importing our TensorFlow libraries\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Import the necessary data science libraries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import the data set as a pandas DataFrame\n",
    "\n",
    "training_data = pd.read_csv('FB_training_data.csv')\n",
    "\n",
    "#Transform the data set into a NumPy array\n",
    "\n",
    "training_data = training_data.iloc[:, 1].values\n",
    "\n",
    "#Apply feature scaling to the data set\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "training_data = scaler.fit_transform(training_data.reshape(-1, 1))\n",
    "\n",
    "#Initialize our x_training_data and y_training_data variables \n",
    "\n",
    "#as empty Python lists\n",
    "\n",
    "x_training_data = []\n",
    "\n",
    "y_training_data =[]\n",
    "\n",
    "#Populate the Python lists using 40 timesteps\n",
    "\n",
    "for i in range(40, len(training_data)):\n",
    "\n",
    "    x_training_data.append(training_data[i-40:i, 0])\n",
    "\n",
    "    y_training_data.append(training_data[i, 0])\n",
    "\n",
    "    \n",
    "\n",
    "#Transforming our lists into NumPy arrays\n",
    "\n",
    "x_training_data = np.array(x_training_data)\n",
    "\n",
    "y_training_data = np.array(y_training_data)\n",
    "\n",
    "#Verifying the shape of the NumPy arrays\n",
    "\n",
    "print(x_training_data.shape)\n",
    "\n",
    "print(y_training_data.shape)\n",
    "\n",
    "#Reshaping the NumPy array to meet TensorFlow standards\n",
    "\n",
    "x_training_data = np.reshape(x_training_data, (x_training_data.shape[0], \n",
    "\n",
    "                                               x_training_data.shape[1], \n",
    "\n",
    "                                               1))\n",
    "\n",
    "#Printing the new shape of x_training_data\n",
    "\n",
    "print(x_training_data.shape)\n",
    "\n",
    "#Importing our TensorFlow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "#Initializing our recurrent neural network\n",
    "\n",
    "rnn = Sequential()\n",
    "\n",
    "#Adding our first LSTM layer\n",
    "\n",
    "rnn.add(LSTM(units = 45, return_sequences = True, input_shape = (x_training_data.shape[1], 1)))\n",
    "\n",
    "#Perform some dropout regularization\n",
    "\n",
    "rnn.add(Dropout(0.2))\n",
    "\n",
    "#Adding three more LSTM layers with dropout regularization\n",
    "\n",
    "for i in [True, True, False]:\n",
    "\n",
    "    rnn.add(LSTM(units = 45, return_sequences = i))\n",
    "\n",
    "    rnn.add(Dropout(0.2))\n",
    "\n",
    "#(Original code for the three additional LSTM layers)\n",
    "\n",
    "# rnn.add(LSTM(units = 45, return_sequences = True))\n",
    "\n",
    "# rnn.add(Dropout(0.2))\n",
    "\n",
    "# rnn.add(LSTM(units = 45, return_sequences = True))\n",
    "\n",
    "# rnn.add(Dropout(0.2))\n",
    "\n",
    "# rnn.add(LSTM(units = 45))\n",
    "\n",
    "# rnn.add(Dropout(0.2))\n",
    "\n",
    "#Adding our output layer\n",
    "\n",
    "rnn.add(Dense(units = 1))\n",
    "\n",
    "#Compiling the recurrent neural network\n",
    "\n",
    "rnn.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "#Training the recurrent neural network\n",
    "\n",
    "rnn.fit(x_training_data, y_training_data, epochs = 100, batch_size = 32)\n",
    "\n",
    "#Import the test data set and transform it into a NumPy array\n",
    "\n",
    "test_data = pd.read_csv('FB_test_data.csv')\n",
    "\n",
    "test_data = test_data.iloc[:, 1].values\n",
    "\n",
    "#Make sure the test data's shape makes sense\n",
    "\n",
    "print(test_data.shape)\n",
    "\n",
    "#Plot the test data\n",
    "\n",
    "plt.plot(test_data)\n",
    "\n",
    "#Create unscaled training data and test data objects\n",
    "\n",
    "unscaled_training_data = pd.read_csv('FB_training_data.csv')\n",
    "\n",
    "unscaled_test_data = pd.read_csv('FB_test_data.csv')\n",
    "\n",
    "#Concatenate the unscaled data\n",
    "\n",
    "all_data = pd.concat((unscaled_training_data['Open'], unscaled_test_data['Open']), axis = 0)\n",
    "\n",
    "#Create our x_test_data object, which has each January day + the 40 prior days\n",
    "\n",
    "x_test_data = all_data[len(all_data) - len(test_data) - 40:].values\n",
    "\n",
    "x_test_data = np.reshape(x_test_data, (-1, 1))\n",
    "\n",
    "#Scale the test data\n",
    "\n",
    "x_test_data = scaler.transform(x_test_data)\n",
    "\n",
    "#Grouping our test data\n",
    "\n",
    "final_x_test_data = []\n",
    "\n",
    "for i in range(40, len(x_test_data)):\n",
    "\n",
    "    final_x_test_data.append(x_test_data[i-40:i, 0])\n",
    "\n",
    "final_x_test_data = np.array(final_x_test_data)\n",
    "\n",
    "#Reshaping the NumPy array to meet TensorFlow standards\n",
    "\n",
    "final_x_test_data = np.reshape(final_x_test_data, (final_x_test_data.shape[0], \n",
    "\n",
    "                                               final_x_test_data.shape[1], \n",
    "\n",
    "                                               1))\n",
    "\n",
    "#Generating our predicted values\n",
    "\n",
    "predictions = rnn.predict(final_x_test_data)\n",
    "\n",
    "#Plotting our predicted values\n",
    "\n",
    "plt.clf() #This clears the old plot from our canvas\n",
    "\n",
    "plt.plot(predictions)\n",
    "\n",
    "#Unscaling the predicted values and re-plotting the data\n",
    "\n",
    "unscaled_predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "plt.clf() #This clears the first prediction plot from our canvas\n",
    "\n",
    "plt.plot(unscaled_predictions)\n",
    "\n",
    "#Plotting the predicted values against Facebook's actual stock price\n",
    "\n",
    "plt.plot(unscaled_predictions, color = '#135485', label = \"Predictions\")\n",
    "\n",
    "plt.plot(test_data, color = 'black', label = \"Real Data\")\n",
    "\n",
    "plt.title('Facebook Stock Price Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc5e8d-284a-4811-bf3b-580cfb59401d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b2fa6-231c-4093-b144-a25795fff262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d18144-e3d0-4246-8c48-4674b184c65b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
